{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cases from EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.4     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.4     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "The data will be written to gs://fc-secure-672eeb92-4859-4ed9-9f59-d4349f3534a0/bq_exports/micah_hysong@researchallofus.org/20250825/condition_87448626/condition_87448626_*.csv. Use this path when reading the data into your notebooks in the future.\n",
      "\n",
      "Loading gs://fc-secure-672eeb92-4859-4ed9-9f59-d4349f3534a0/bq_exports/micah_hysong@researchallofus.org/20250825/condition_87448626/condition_87448626_000000000000.csv.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>360831</li><li>6</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 360831\n",
       "\\item 6\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 360831\n",
       "2. 6\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 360831      6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"prostate_cancer_cases\" for domain \"condition\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_87448626_condition_sql <- paste(\"\n",
    "    SELECT\n",
    "        c_occurrence.person_id,\n",
    "        c_occurrence.condition_concept_id,\n",
    "        c_standard_concept.concept_name as standard_concept_name,\n",
    "        c_standard_concept.concept_code as standard_concept_code,\n",
    "        c_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        c_occurrence.condition_start_datetime \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `condition_occurrence` c_occurrence \n",
    "        WHERE\n",
    "            (\n",
    "                condition_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (200962, 36712762, 37016740, 4163261, 4196262)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )  \n",
    "            AND (\n",
    "                c_occurrence.PERSON_ID IN (SELECT\n",
    "                    distinct person_id  \n",
    "                FROM\n",
    "                    `cb_search_person` cb_search_person  \n",
    "                WHERE\n",
    "                    cb_search_person.person_id IN (SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_whole_genome_variant = 1 ) \n",
    "                    AND cb_search_person.person_id IN (SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_ehr_data = 1 ) )\n",
    "            )) c_occurrence \n",
    "    LEFT JOIN\n",
    "        `concept` c_standard_concept \n",
    "            ON c_occurrence.condition_concept_id = c_standard_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "condition_87448626_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"condition_87448626\",\n",
    "  \"condition_87448626_*.csv\")\n",
    "message(str_glue('The data will be written to {condition_87448626_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_87448626_condition_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  condition_87448626_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {condition_87448626_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "condition_df <- read_bq_export_from_workspace_bucket(condition_87448626_path)\n",
    "\n",
    "dim(condition_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'Prostate cancer metastatic to bone'</li><li>'Metastatic castration-resistant prostate cancer'</li><li>'Malignant tumor of prostate'</li><li>'Secondary malignant neoplasm of prostate'</li><li>'Carcinoma of prostate'</li><li>'Metastasis from malignant tumor of prostate'</li><li>'Adenocarcinoma of prostate'</li><li>'Hormone refractory prostate cancer'</li><li>'Recurrent malignant neoplasm of prostate'</li><li>'Primary malignant neoplasm of prostate'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Prostate cancer metastatic to bone'\n",
       "\\item 'Metastatic castration-resistant prostate cancer'\n",
       "\\item 'Malignant tumor of prostate'\n",
       "\\item 'Secondary malignant neoplasm of prostate'\n",
       "\\item 'Carcinoma of prostate'\n",
       "\\item 'Metastasis from malignant tumor of prostate'\n",
       "\\item 'Adenocarcinoma of prostate'\n",
       "\\item 'Hormone refractory prostate cancer'\n",
       "\\item 'Recurrent malignant neoplasm of prostate'\n",
       "\\item 'Primary malignant neoplasm of prostate'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Prostate cancer metastatic to bone'\n",
       "2. 'Metastatic castration-resistant prostate cancer'\n",
       "3. 'Malignant tumor of prostate'\n",
       "4. 'Secondary malignant neoplasm of prostate'\n",
       "5. 'Carcinoma of prostate'\n",
       "6. 'Metastasis from malignant tumor of prostate'\n",
       "7. 'Adenocarcinoma of prostate'\n",
       "8. 'Hormone refractory prostate cancer'\n",
       "9. 'Recurrent malignant neoplasm of prostate'\n",
       "10. 'Primary malignant neoplasm of prostate'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Prostate cancer metastatic to bone\"             \n",
       " [2] \"Metastatic castration-resistant prostate cancer\"\n",
       " [3] \"Malignant tumor of prostate\"                    \n",
       " [4] \"Secondary malignant neoplasm of prostate\"       \n",
       " [5] \"Carcinoma of prostate\"                          \n",
       " [6] \"Metastasis from malignant tumor of prostate\"    \n",
       " [7] \"Adenocarcinoma of prostate\"                     \n",
       " [8] \"Hormone refractory prostate cancer\"             \n",
       " [9] \"Recurrent malignant neoplasm of prostate\"       \n",
       "[10] \"Primary malignant neoplasm of prostate\"         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(condition_df$standard_concept_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df_counts <- condition_df |>\n",
    "    group_by(person_id) |>\n",
    "    summarize(number_counts = n_distinct(condition_start_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "6180"
      ],
      "text/latex": [
       "6180"
      ],
      "text/markdown": [
       "6180"
      ],
      "text/plain": [
       "[1] 6180"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case_definition1 <- condition_df_counts[condition_df_counts$number_counts >=2, ]\n",
    "nrow(case_definition1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cases from Self-reported History Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The data will be written to gs://fc-secure-672eeb92-4859-4ed9-9f59-d4349f3534a0/bq_exports/micah_hysong@researchallofus.org/20250825/survey_81293648/survey_81293648_*.csv. Use this path when reading the data into your notebooks in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"pc\" for domain \"survey\" and was generated for All of Us Controlled Tier Dataset v8\n",
    "dataset_81293648_survey_sql <- paste(\"\n",
    "    SELECT\n",
    "        answer.person_id,\n",
    "        answer.survey_datetime,\n",
    "        answer.survey,\n",
    "        answer.question_concept_id,\n",
    "        answer.question,\n",
    "        answer.answer_concept_id,\n",
    "        answer.answer,\n",
    "        answer.survey_version_concept_id,\n",
    "        answer.survey_version_name  \n",
    "    FROM\n",
    "        `ds_survey` answer   \n",
    "    WHERE\n",
    "        (\n",
    "            question_concept_id IN (836780)\n",
    "        )  \n",
    "        AND (\n",
    "            answer.PERSON_ID IN (SELECT\n",
    "                distinct person_id  \n",
    "            FROM\n",
    "                `cb_search_person` cb_search_person  \n",
    "            WHERE\n",
    "                cb_search_person.person_id IN (SELECT\n",
    "                    person_id \n",
    "                FROM\n",
    "                    `cb_search_person` p \n",
    "                WHERE\n",
    "                    has_whole_genome_variant = 1 ) \n",
    "                AND cb_search_person.person_id IN (SELECT\n",
    "                    person_id \n",
    "                FROM\n",
    "                    `cb_search_person` p \n",
    "                WHERE\n",
    "                    has_ehr_data = 1 ) )\n",
    "        )\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "survey_81293648_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"survey_81293648\",\n",
    "  \"survey_81293648_*.csv\")\n",
    "message(str_glue('The data will be written to {survey_81293648_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_81293648_survey_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  survey_81293648_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {survey_81293648_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character(), survey_version_name = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "survey_df <- read_bq_export_from_workspace_bucket(survey_81293648_path)\n",
    "\n",
    "dim(survey_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc<-survey_df[survey_df$answer == \"Including yourself, who in your family has had prostate cancer? - Self\",]\n",
    "case_definition2<-unique(pc$person_id)\n",
    "length(case_definition2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(case_definition1)\n",
    "length(unique(case_definition1$person_id))\n",
    "length(case_definition2)\n",
    "full_list<-c(case_definition1$person_id, case_definition2)\n",
    "cases<-unique(full_list)\n",
    "length(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"prostatectomy\" for domain \"procedure\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_47173302_procedure_sql <- paste(\"\n",
    "    SELECT\n",
    "        procedure.person_id,\n",
    "        procedure.procedure_concept_id,\n",
    "        p_standard_concept.concept_name as standard_concept_name,\n",
    "        p_standard_concept.concept_code as standard_concept_code,\n",
    "        p_standard_concept.vocabulary_id as standard_vocabulary \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `procedure_occurrence` procedure \n",
    "        WHERE\n",
    "            (\n",
    "                procedure_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (4073700, 4211496, 4235738)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )  \n",
    "            AND (\n",
    "                procedure.PERSON_ID IN (SELECT\n",
    "                    distinct person_id  \n",
    "                FROM\n",
    "                    `cb_search_person` cb_search_person  \n",
    "                WHERE\n",
    "                    cb_search_person.person_id IN (SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_whole_genome_variant = 1 ) \n",
    "                    AND cb_search_person.person_id IN (SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_ehr_data = 1 ) )\n",
    "            )) procedure \n",
    "    LEFT JOIN\n",
    "        `concept` p_standard_concept \n",
    "            ON procedure.procedure_concept_id = p_standard_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "procedure_47173302_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"procedure_47173302\",\n",
    "  \"procedure_47173302_*.csv\")\n",
    "message(str_glue('The data will be written to {procedure_47173302_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_47173302_procedure_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  procedure_47173302_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {procedure_47173302_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "procedure_df <- read_bq_export_from_workspace_bucket(procedure_47173302_path)\n",
    "\n",
    "exclude1<-unique(procedure_df$person_id)\n",
    "length(exclude1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"prostate_psa\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_05885865_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (40779420, 4272032)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )  \n",
    "            AND (\n",
    "                measurement.PERSON_ID IN (SELECT\n",
    "                    distinct person_id  \n",
    "                FROM\n",
    "                    `cb_search_person` cb_search_person  \n",
    "                WHERE\n",
    "                    cb_search_person.person_id IN (SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_whole_genome_variant = 1 ) \n",
    "                    AND cb_search_person.person_id IN (SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_ehr_data = 1 ) )\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_05885865_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_05885865\",\n",
    "  \"measurement_05885865_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_05885865_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_05885865_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_05885865_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_05885865_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "has_psa <- read_bq_export_from_workspace_bucket(measurement_05885865_path)\n",
    "\n",
    "dim(has_psa)\n",
    "length(unique(has_psa$person_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls<-setdiff(has_psa$person_id, exclude1)\n",
    "controls<-setdiff(controls, cases)\n",
    "length(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases <- data.frame(\n",
    "  person_id = cases,\n",
    "  status = 1\n",
    ")\n",
    "\n",
    "df_controls <- data.frame(\n",
    "  person_id = controls,\n",
    "  status = 0\n",
    ")\n",
    "\n",
    "final_df <- rbind(df_cases, df_controls)\n",
    "nrow(final_df)\n",
    "n_distinct(final_df$person_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove cis_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet assumes that you run setup first\n",
    "\n",
    "# This code copies a file from your Google Bucket into a dataframe\n",
    "\n",
    "# replace 'test.csv' with the name of the file in your google bucket (don't delete the quotation marks)\n",
    "name_of_file_in_bucket <- 'Demographic_and_ancestry_covariates.csv'\n",
    "\n",
    "########################################################################\n",
    "##\n",
    "################# DON'T CHANGE FROM HERE ###############################\n",
    "##\n",
    "########################################################################\n",
    "\n",
    "# Get the bucket name\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# Copy the file from current workspace to the bucket\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/data/\", name_of_file_in_bucket, \" .\"), intern=T)\n",
    "\n",
    "# Load the file into a dataframe\n",
    "demographics  <- read_csv(name_of_file_in_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_female_ids <- demographics %>%\n",
    "  filter(SexGender == \"Cis_female\") %>%\n",
    "  select(person_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df <- final_df[!(final_df$person_id %in% cis_female_ids$person_id), ]\n",
    "n_distinct(final_df$person_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet assumes that you run setup first\n",
    "\n",
    "# This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
    "\n",
    "# Replace df with THE NAME OF YOUR DATAFRAME\n",
    "my_dataframe <- final_df\n",
    "\n",
    "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
    "destination_filename <- 'eMERGE_prostate_cancer_case_control_df.csv'\n",
    "\n",
    "########################################################################\n",
    "##\n",
    "################# DON'T CHANGE FROM HERE ###############################\n",
    "##\n",
    "########################################################################\n",
    "\n",
    "# store the dataframe in current workspace\n",
    "write_excel_csv(my_dataframe, destination_filename)\n",
    "\n",
    "# Get the bucket name\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# Copy the file from current workspace to the bucket\n",
    "system(paste0(\"gsutil cp ./\", destination_filename, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "\n",
    "# Check if file is in the bucket\n",
    "system(paste0(\"gsutil ls \", my_bucket, \"/data/*.csv\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
